services:
  vllm:
    image: ${IMAGE:-vllm/vllm-openai:latest}
    container_name: vllm-rag
    runtime: nvidia
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - LD_LIBRARY_PATH=/usr/lib/wsl/lib:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - VLLM_ATTENTION_BACKEND=FLASH_ATTN
      - VLLM_API_KEY=${VLLM_API_KEY:-}
    ports:
      - "${PORT:-8888}:8888"
    volumes:
      - /usr/lib/wsl:/usr/lib/wsl
      - ~/.cache/huggingface:/root/.cache/huggingface
    devices:
      - /dev/dxg
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    entrypoint: /bin/bash
    command: >
      -c "sed -i 's/return \"microsoft\" in \" \".join(platform.uname()).lower()/return False/g' \$(python3 -c 'import vllm; import os; print(os.path.join(os.path.dirname(vllm.__file__), \"platforms/interface.py\"))') && \
          python3 -c \"import vllm.entrypoints.openai.api_server as api; path = api.__file__; content = open(path).read(); (open(path, 'w').write(content.replace('app.include_router(router)', 'app.include_router(router)\\n\\n    @app.get(\\\"/\\\")\\n    async def root(): return {\\\"status\\\": \\\"ok\\\", \\\"message\\\": \\\"vLLM API Server (RAG) is running\\\", \\\"version\\\": VLLM_VERSION}'))) if '@app.get(\\\"/\\\")' not in content else None\" && \
          exec vllm serve \"${MODEL:-Qwen/Qwen2.5-Coder-7B-Instruct-AWQ}\" --host 0.0.0.0 --port 8888 --gpu-memory-utilization 0.7 --max-model-len 16384 $${VLLM_API_KEY:+--api-key \"$$VLLM_API_KEY\"}"
    restart: "no"

  qdrant:
    image: qdrant/qdrant:v1.14.0
    container_name: qdrant-rag
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./data/qdrant:/qdrant/storage
    restart: "no"

  infinity:
    image: michaelf34/infinity:latest
    container_name: infinity-embeddings
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - LD_LIBRARY_PATH=/usr/lib/wsl/lib:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
    ports:
      - "7997:7997"
    volumes:
      - /usr/lib/wsl:/usr/lib/wsl
      - ~/.cache/huggingface:/root/.cache/huggingface
    devices:
      - /dev/dxg
    command: v2 --model-id BAAI/bge-m3 --batch-size 32 --device cuda --no-bettertransformer --no-compile --url-prefix /v1
    restart: "no"

  db:
    image: postgres:16-alpine
    container_name: postgres-rag
    ports:
      - "5438:5432"
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-webui}
      - POSTGRES_USER=${POSTGRES_USER:-webui}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-webui}
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    restart: "no"

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui-rag
    ports:
      - "3005:8080"
    environment:
      - OPENAI_API_BASE_URLS=http://vllm:8888/v1
      - OPENAI_API_KEYS=${VLLM_API_KEY:-sk-dummy}
      - RAG_EMBEDDING_ENGINE=openai
      - RAG_OPENAI_API_BASE_URL=http://infinity:7997/v1
      - RAG_OPENAI_API_KEY=sk-dummy
      - RAG_EMBEDDING_MODEL=BAAI/bge-m3
      - VECTOR_DB=qdrant
      - QDRANT_URI=http://qdrant:6333
      - QDRANT_HOST=qdrant
      - DATABASE_URL=postgresql://${POSTGRES_USER:-webui}:${POSTGRES_PASSWORD:-webui}@db:5432/${POSTGRES_DB:-webui}
      - USER_AGENT=OpenWebUI
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
      - CORS_ALLOW_ORIGIN=${CORS_ALLOW_ORIGIN}
    volumes:
      - ./data/webui:/app/backend/data
    depends_on:
      - vllm
      - qdrant
      - infinity
      - db
    restart: "no"

  ingest-engine:
    build: ./ingest
    container_name: rag-ingest-engine
    volumes:
      - ./storage/inbox:/app/inbox
      - ./storage/processed:/app/processed
      - ./ingest/src:/app/src
    environment:
      - QDRANT_URI=http://qdrant:6333
      - QDRANT_HOST=qdrant
      - EMBEDDING_API_URL=http://infinity:7997/v1
      - EMBEDDING_MODEL=${RAG_EMBEDDING_MODEL:-BAAI/bge-m3}
      - LLM_API_URL=http://vllm:8888/v1
      - LLM_MODEL=${MODEL:-Qwen/Qwen3-0.6B}
      - LLM_API_KEY=${VLLM_API_KEY:-sk-dummy}
    restart: "no"
